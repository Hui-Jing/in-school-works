{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# November 5, 2019 lecture notes\n",
    "\n",
    "This week we'll take a first look at [Apache Spark](http://spark.apache.org/), a powerful system for scaling up data processing.\n",
    "\n",
    "Spark is installed on your instances, but requires a few steps to use.  Every time you use Spark, you'll have to run the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two steps are required for the simple Python style of using Spark.  We'll see one other approach later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(appName='20191105')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spark` object is all you need to start doing some pretty cool things.  Let's have a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-83-234:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>20191105</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=20191105>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have port 4040 open in your EC2 security groups, you can follow the link above to the Spark instance UI on your machine.  There's one caveat though - it maps automatically to an IP address that won't work unless you take extra steps.  The solution is simple:  replace the IP address in the URL with your machine name, the one that starts with \"ec2-\" that you are probably looking at Jupyter through right now.\n",
    "\n",
    "**Note**, though, the Spark UI defaults to port 4040, not 8080 like Jupyter, so **be sure you use `:4040`** in your URL.\n",
    "\n",
    "Have a look now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bikeshare data\n",
    "\n",
    "Because Capital Bikeshare data is so familiar, let's have a look at it using Spark.  Maybe you've already seen what it looks like at the command line, using CSVKit and XSV, and with SQL.  Now we'll try a functional style of approach.\n",
    "\n",
    "These first few steps should be familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-12 04:30:28--  https://s3.amazonaws.com/capitalbikeshare-data/2017-capitalbikeshare-tripdata.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.16.83\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.16.83|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 89576218 (85M) [application/zip]\n",
      "Saving to: ‘2017-capitalbikeshare-tripdata.zip’\n",
      "\n",
      "2017-capitalbikesha 100%[===================>]  85.43M  78.2MB/s    in 1.1s    \n",
      "\n",
      "2019-11-12 04:30:30 (78.2 MB/s) - ‘2017-capitalbikeshare-tripdata.zip’ saved [89576218/89576218]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/capitalbikeshare-data/2017-capitalbikeshare-tripdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ubuntu ubuntu 86M Mar 15  2018 2017-capitalbikeshare-tripdata.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -lh 2017-capitalbikeshare-tripdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2017-capitalbikeshare-tripdata.zip\n",
      "  inflating: 2017Q1-capitalbikeshare-tripdata.csv  \n",
      "  inflating: 2017Q2-capitalbikeshare-tripdata.csv  \n",
      "  inflating: 2017Q3-capitalbikeshare-tripdata.csv  \n",
      "  inflating: 2017Q4-capitalbikeshare-tripdata.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip 2017-capitalbikeshare-tripdata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   646511 2017Q1-capitalbikeshare-tripdata.csv\n",
      "  1104419 2017Q2-capitalbikeshare-tripdata.csv\n",
      "  1191586 2017Q3-capitalbikeshare-tripdata.csv\n",
      "   815265 2017Q4-capitalbikeshare-tripdata.csv\n",
      "  3757781 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l 20*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration\n",
      "  2: Start date\n",
      "  3: End date\n",
      "  4: Start station number\n",
      "  5: Start station\n",
      "  6: End station number\n",
      "  7: End station\n",
      "  8: Bike number\n",
      "  9: Member type\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n 2017Q1-capitalbikeshare-tripdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration\n",
      "  2: Start date\n",
      "  3: End date\n",
      "  4: Start station number\n",
      "  5: Start station\n",
      "  6: End station number\n",
      "  7: End station\n",
      "  8: Bike number\n",
      "  9: Member type\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n 2017Q3-capitalbikeshare-tripdata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVkit and XSV\n",
    "\n",
    "To keep these command line pipes simple, let's combine and rename the data into one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp 2017Q1-capitalbikeshare-tripdata.csv 2017.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head 2017.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -lh *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wc -l *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, then, we have 429M of raw data comprising 3,757,777 bikeshare trips.  How long does it take to sort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look at a sample of the data to determine its attributes' domains and ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1000 2017.csv | csvstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time os.system(\"head -25000 2017.csv | csvsort -c1 | head | csvlook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it faster if we sort only one column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time os.system(\"head -25000 2017.csv | csvcut -c1 | csvsort -c1 | head | csvlook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about if we sort using unix `sort` instead of `csvsort`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time os.system(\"head -25000 2017.csv | csvcut -c1 | sort | head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is `sort` faster than `csvsort`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `xsv`\n",
    "\n",
    "[`xsv`](https://github.com/BurntSushi/xsv) is another CSV toolkit like CSVKit, but where CSVKit was designed to be easy and consistent, `xsv` was designed to be *fast*.  And it's really fast.\n",
    "\n",
    "It has a lot of functions similar to CSVKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head 2017.csv | xsv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv headers 2017.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv search -s5 \"Eastern Market / 7th\" 2017.csv | xsv select 1,5,7 | head | xsv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv select 4 2017.csv | head | xsv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv select 4 2017.csv | xsv frequency | xsv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv select 1 2017.csv | xsv stats | xsv table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv sort -s4 2017.csv | head | xsv table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seemed pretty fast... how fast was it?  Let's try to reproduce our test from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time os.system(\"xsv select 4 2017.csv | xsv sort | uniq -c | head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  That's really fast.  Even faster than unix `sort`!  Let's use `%timeit` to try repeated runs and get a better sample for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit os.system(\"xsv select 4 2017.csv | xsv sort | uniq -c | head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit os.system(\"xsv select 4 2017.csv | sort | uniq -c | head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's run that again with `csvcut` instead of `xsv select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit os.system(\"csvcut -c4 2017.csv | sort | uniq -c | head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty impressive, right?  It was designed to be *fast*.  And it is.\n",
    "\n",
    "But wait, there's more.  `xsv` supports *indexing*, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv index 2017.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -lh 2017*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!xsv select 5 2017.csv | xsv frequency | xsv table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral of the story:  different tools with similar goals but different designs can each both achieve their goals effectively.  Knowing which to choose for a particular task requires an understanding of the design tradeoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping vs. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization (or array programming) is a critical piece of the data science puzzle.  Fast implementations of array operations take advantage of low-level hardware to make operations on matrixes very fast, which is critical for machine learning and other statistical operations on large datasets.\n",
    "\n",
    "To get a taste of the difference vectorized operations work, let's look at a simple function:  finding the largest value in a matrix.  This should have complexity O(n), or linear, as the number of compute operations increases linearly with the size of the input set.\n",
    "\n",
    "We can use the [pyheatmagic](https://github.com/csurfer/pyheatmagic) Jupyter extension to look at the performance of two versions of a function that find the largest value within a matrix of random numbers.  The larger the matrix size, the larger the result should be - or in this case, the closer to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%heat\n",
    "import random\n",
    "def arraymax1(m, n):\n",
    "    ints = [random.random() for n in range(m * n)]\n",
    "    maxval = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            index = (i * n) + j\n",
    "            if ints[index] > maxval:\n",
    "                maxval = ints[index]\n",
    "    return maxval\n",
    "\n",
    "print(arraymax1(300, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%heat\n",
    "import numpy as np\n",
    "def arraymax2(m, n):\n",
    "    a = np.random.random(m * n).reshape(m, n)\n",
    "    return np.max(a)\n",
    "\n",
    "print(arraymax2(300, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def arraymax1(m, n):\n",
    "    ints = [random.random() for n in range(m * n)]\n",
    "    maxval = 0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            index = (i * n) + j\n",
    "            if ints[index] > maxval:\n",
    "                maxval = ints[index]\n",
    "    return maxval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def arraymax2(m, n):\n",
    "    a = np.random.random(m * n).reshape(m, n)\n",
    "    return np.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit arraymax1(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%timeit arraymax2(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data with Spark\n",
    "\n",
    "To get started, we identify the data we want to work with, in this case one of the unstacked/raw CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides1 = spark.textFile('2017Q1-capitalbikeshare-tripdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rides1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right so far.  Two more details:  first, we could load the second file the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides2 = spark.textFile('2017Q2-capitalbikeshare-tripdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rides2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that seems so tedious.  And besides, Spark makes this easier:  you can use wildcards to load more than one file at a time into a single RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = spark.textFile('2017Q*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 8 ms, total: 12 ms\n",
      "Wall time: 11.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3757781"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rides.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good moment to pause and make sure you understand what you've done so far.  We've loaded one or more than one text file into a Spark RDD and used parallel processing to count the number of lines in the file.  If it doesn't seem like you've done that much, take a look at the Spark UI now.\n",
    "\n",
    "Really, take a look!\n",
    "\n",
    "Ready to continue?\n",
    "\n",
    "Okay, let's keep going with a look at the functional style of RDD processing.  This is pretty natural to someone who's used functional languages or is used to doing a lot of list processing in Python.\n",
    "\n",
    "`first()` will extract the first line of the file, which, in this case, contains our headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Duration\",\"Start date\",\"End date\",\"Start station number\",\"Start station\",\"End station number\",\"End station\",\"Bike number\",\"Member type\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = rides.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to jump ahead and do a bunch of things at once, and then break them all down so you can see the steps one at a time.\n",
    "\n",
    "First we load the `add` function; this is a useful shorthand we'll use in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works just like you'd expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add(add(add(add(add(add(1, 1), 1), 2), 3), 5), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sense?  Good. \n",
    "\n",
    "Okay, now the leap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70062\tColumbus Circle / Union Station\n",
      "65884\tLincoln Memorial\n",
      "59259\tJefferson Dr & 14th St SW\n",
      "46702\tMassachusetts Ave & Dupont Circle NW\n",
      "43305\t15th & P St NW\n",
      "42525\tJefferson Memorial\n",
      "42406\tSmithsonian-National Mall / Jefferson Dr & 12th St SW\n",
      "40659\tHenry Bacon Dr & Lincoln Memorial Circle NW\n",
      "37751\t4th St & Madison Dr NW\n",
      "33159\t14th & V St NW\n"
     ]
    }
   ],
   "source": [
    "top10 = rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[4], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])\n",
    "for station, count in top10:\n",
    "    print(\"{}\\t{}\".format(count, station))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look familiar?  Good.  Let's break it down.\n",
    "\n",
    "First, we use `filter()`, which does just what you expect it to do.  It lets some things through, but not others.  In this case we use `lambda`, or an \"anonymous function\", to remove the header from the stream.\n",
    "\n",
    "`take()` lets us extract values from the RDD - remember that the RDD is just a logical construct until we materialize some sort of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Duration\",\"Start date\",\"End date\",\"Start station number\",\"Start station\",\"End station number\",\"End station\",\"Bike number\",\"Member type\"',\n",
       " '\"2762\",\"2017-07-01 00:01:09\",\"2017-07-01 00:47:11\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"W21474\",\"Casual\"',\n",
       " '\"2763\",\"2017-07-01 00:01:24\",\"2017-07-01 00:47:27\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"W22042\",\"Casual\"',\n",
       " '\"690\",\"2017-07-01 00:01:45\",\"2017-07-01 00:13:16\",\"31122\",\"16th & Irving St NW\",\"31299\",\"Connecticut Ave & R St NW\",\"W01182\",\"Member\"',\n",
       " '\"134\",\"2017-07-01 00:01:46\",\"2017-07-01 00:04:00\",\"31201\",\"15th & P St NW\",\"31267\",\"17th St & Massachusetts Ave NW\",\"W22829\",\"Member\"']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same thing with the header filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"2762\",\"2017-07-01 00:01:09\",\"2017-07-01 00:47:11\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"W21474\",\"Casual\"',\n",
       " '\"2763\",\"2017-07-01 00:01:24\",\"2017-07-01 00:47:27\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"31289\",\"Henry Bacon Dr & Lincoln Memorial Circle NW\",\"W22042\",\"Casual\"',\n",
       " '\"690\",\"2017-07-01 00:01:45\",\"2017-07-01 00:13:16\",\"31122\",\"16th & Irving St NW\",\"31299\",\"Connecticut Ave & R St NW\",\"W01182\",\"Member\"',\n",
       " '\"134\",\"2017-07-01 00:01:46\",\"2017-07-01 00:04:00\",\"31201\",\"15th & P St NW\",\"31267\",\"17th St & Massachusetts Ave NW\",\"W22829\",\"Member\"',\n",
       " '\"587\",\"2017-07-01 00:02:05\",\"2017-07-01 00:11:52\",\"31099\",\"Madison & N Henry St\",\"31907\",\"Franklin & S Washington St\",\"W22223\",\"Casual\"']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row!= header) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference?  Great.  We're only filtering out the header line.\n",
    "\n",
    "But now all we have is a list of strings, which isn't very useful if we want to operate on the data.  Next, then, we'll remove `\"` marks and split the CSV data up by commas using `map`, which applies a function to every row in the RDD.  We'll define a lambda function right inline again, as it's easy, although you could just as easily use a regular named Python function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2762',\n",
       "  '2017-07-01 00:01:09',\n",
       "  '2017-07-01 00:47:11',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  'W21474',\n",
       "  'Casual'],\n",
       " ['2763',\n",
       "  '2017-07-01 00:01:24',\n",
       "  '2017-07-01 00:47:27',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  'W22042',\n",
       "  'Casual'],\n",
       " ['690',\n",
       "  '2017-07-01 00:01:45',\n",
       "  '2017-07-01 00:13:16',\n",
       "  '31122',\n",
       "  '16th & Irving St NW',\n",
       "  '31299',\n",
       "  'Connecticut Ave & R St NW',\n",
       "  'W01182',\n",
       "  'Member'],\n",
       " ['134',\n",
       "  '2017-07-01 00:01:46',\n",
       "  '2017-07-01 00:04:00',\n",
       "  '31201',\n",
       "  '15th & P St NW',\n",
       "  '31267',\n",
       "  '17th St & Massachusetts Ave NW',\n",
       "  'W22829',\n",
       "  'Member'],\n",
       " ['587',\n",
       "  '2017-07-01 00:02:05',\n",
       "  '2017-07-01 00:11:52',\n",
       "  '31099',\n",
       "  'Madison & N Henry St',\n",
       "  '31907',\n",
       "  'Franklin & S Washington St',\n",
       "  'W22223',\n",
       "  'Casual']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we'd define and use a named Python function to accomplish the CSV splitting steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(line, sep=','):\n",
    "    return line.replace('\"', '').split(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2762',\n",
       "  '2017-07-01 00:01:09',\n",
       "  '2017-07-01 00:47:11',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  'W21474',\n",
       "  'Casual'],\n",
       " ['2763',\n",
       "  '2017-07-01 00:01:24',\n",
       "  '2017-07-01 00:47:27',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  '31289',\n",
       "  'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       "  'W22042',\n",
       "  'Casual'],\n",
       " ['690',\n",
       "  '2017-07-01 00:01:45',\n",
       "  '2017-07-01 00:13:16',\n",
       "  '31122',\n",
       "  '16th & Irving St NW',\n",
       "  '31299',\n",
       "  'Connecticut Ave & R St NW',\n",
       "  'W01182',\n",
       "  'Member'],\n",
       " ['134',\n",
       "  '2017-07-01 00:01:46',\n",
       "  '2017-07-01 00:04:00',\n",
       "  '31201',\n",
       "  '15th & P St NW',\n",
       "  '31267',\n",
       "  '17th St & Massachusetts Ave NW',\n",
       "  'W22829',\n",
       "  'Member'],\n",
       " ['587',\n",
       "  '2017-07-01 00:02:05',\n",
       "  '2017-07-01 00:11:52',\n",
       "  '31099',\n",
       "  'Madison & N Henry St',\n",
       "  '31907',\n",
       "  'Franklin & S Washington St',\n",
       "  'W22223',\n",
       "  'Casual']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row != header) \\\n",
    "    .map(split_line) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the two functions are equivalent?  The `map()` function will pass the data as the first parameter to the `split_lines()` function we defined.\n",
    "\n",
    "I think `lambda` reads a little cleaner for simpler operations, so we'll continue with them for now.\n",
    "\n",
    "Okay!  Now we're cooking.  Next, let's pull out the departure stations.  Which column was it again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Duration\",\"Start date\",\"End date\",\"Start station number\",\"Start station\",\"End station number\",\"End station\",\"Bike number\",\"Member type\"'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python's zero-based indexing, the \"Start station\" column is number 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       " 'Henry Bacon Dr & Lincoln Memorial Circle NW',\n",
       " '16th & Irving St NW',\n",
       " '15th & P St NW',\n",
       " 'Madison & N Henry St']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: cols[4]) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works - but we want to count them, so we'll need a numeric value to count.  Thus the tuple with \"`, 1`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Henry Bacon Dr & Lincoln Memorial Circle NW', 1),\n",
       " ('Henry Bacon Dr & Lincoln Memorial Circle NW', 1),\n",
       " ('16th & Irving St NW', 1),\n",
       " ('15th & P St NW', 1),\n",
       " ('Madison & N Henry St', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[4], 1)) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure sets us up neatly for another Spark function, `reduceByKey()`.  This is an addition from Spark, whereas `map()`, `filter()`, and `lambda` are all standard Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anacostia Library', 709),\n",
       " ('15th & Crystal Dr', 5516),\n",
       " ('Key West Ave & Diamondback Dr', 23),\n",
       " ('King St Metro South', 4714),\n",
       " ('River Rd & Landy Ln', 3427)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[4], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that that took a little longer? \n",
    "\n",
    "Can you guess why?\n",
    "\n",
    "Okay, so, this looks pretty good.  But there's one issue with the data, do you see it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Columbus Circle / Union Station', 70062),\n",
       " ('Lincoln Memorial', 65884),\n",
       " ('Jefferson Dr & 14th St SW', 59259),\n",
       " ('Massachusetts Ave & Dupont Circle NW', 46702),\n",
       " ('15th & P St NW', 43305),\n",
       " ('Jefferson Memorial', 42525),\n",
       " ('Smithsonian-National Mall / Jefferson Dr & 12th St SW', 42406),\n",
       " ('Henry Bacon Dr & Lincoln Memorial Circle NW', 40659),\n",
       " ('4th St & Madison Dr NW', 37751),\n",
       " ('14th & V St NW', 33159)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[4], 1)) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])\n",
    "top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `top10` is now a Python list, not an RDD.  When we `take` or `takeOrdered` (or `collect` or others) we act on the RDD using the logic we've built up and end up with regular Python data structures.  Until then, we still just have an RDD with more operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_top10 = rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\")) \\\n",
    "    .map(lambda cols: (cols[4], 1)) \\\n",
    "    .reduceByKey(add)\n",
    "type(rdd_top10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_top10.takeOrdered(5, key=lambda r: -r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_top10.takeOrdered(10, key=lambda r: -r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing basic statistics\n",
    "\n",
    "Let's go a step further.  `csvstat` and `xsv stats` / `xsv frequency` are so useful, you'd expect there to be something similar for Spark, right?  Of course there is.\n",
    "\n",
    "First let's create an RDD of just the parsed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_data = rides.filter(lambda row: row != header) \\\n",
    "    .map(lambda row: row.replace('\"', '')) \\\n",
    "    .map(lambda row: row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:49"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ride_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract the ride durations in minutes.  Remember how we did this in SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes = ride_data.map(lambda cols: int(cols[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2762, 2763, 690, 134, 587]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ride_minutes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's [MLlib](https://spark.apache.org/mllib/) is the foundation for a lot of machine learning functionality.  A simple module inside it computes basic statistics.  It has a very creative name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this module, we need to convert our data values into numpy arrays, which is just an easy `map()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes_stats = Statistics.colStats(ride_minutes.map(lambda r: np.array(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes_stats.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes_stats.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes_stats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ride_minutes_stats.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ride_minutes_stats.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame API\n",
    "\n",
    "Let's start with the DataFrame API.  It's for array-oriented operations, just like you might already be used to with R or Python's Pandas module.\n",
    "\n",
    "Note that you can find some introductory docs for this and the SQL API on the [Apache Spark docs page](https://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "The first step is that we load data a little differently.  We'll step away from the bikes this week and look at something else:  social media data from Twitter.\n",
    "\n",
    "Note that the data in these examples, and more data you can obtain for yourself, came from the GWU Libraries' [Social Feed Manager](https://sfm.library.gwu.edu/) app.  You can log in and use it yourself, though note that access is restricted to campus or VPN connections.\n",
    "\n",
    "First we obtain a `SQLContext` from our existing `SparkContext`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/week-9/solar-eclipse-tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv solar-eclipse-tweets.csv tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head tweets.csv | csvcut -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 tweets.csv | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read.csv()` function on `SQLContext` is very handy.  Take a close look at the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = sqlc.read.csv(\"tweets.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like what we've seen before, yes?  Except that these are `Rows`, not an RDD.\n",
    "\n",
    "They **do** have an RDD under the hood, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a little more with a `DataFrame` than you can with an `RDD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all well and good, but how well did schema inference work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very well!  This is not uncommon.  You might have to cast some columns to other types, like in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "dir(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.withColumn(\"created_at\", tweets[\"created_at\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.select('created_at').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All fixed!\n",
    "\n",
    "Note that you can define a full schema at load time to avoid this problem.  It would be good if `inferSchema()` were a little more reliable though, although as we'll see in a minute, our data isn't exactly clean.\n",
    "\n",
    "\n",
    "### Operations on DataFrames\n",
    "\n",
    "DataFrames support many of the kinds of df operations you're used to, they are all just a little different.  Use the docs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.describe('followers_count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, looks like we've got some slop in our data.  This might be due to some strange characters in the mix.  Clean that up in a handy wrangling tool..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.select(\"screen_name\", \"text\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.filter(\"followers_count > 15000\") \\\n",
    "    .select(\"followers_count\") \\\n",
    "    .orderBy(\"followers_count\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, looks like another data type problem.  We can fix that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "tweets = tweets.withColumn(\"followers_count\", tweets[\"followers_count\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.filter(\"followers_count > 15000\") \\\n",
    "    .select(\"followers_count\") \\\n",
    "    .orderBy(\"followers_count\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are these popular tweeters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.filter(\"followers_count > 5000000\").select(\"screen_name\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://twitter.com/people\n",
    "\n",
    "Yep - that looks about right.\n",
    "\n",
    "Now that we have that column sorted out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.describe(\"followers_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.orderBy(\"created_at\", ascending=False).select(\"created_at\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe-like operations (with '[]')\n",
    "\n",
    "We can also write code that looks a lot more like Pandas in Python or R data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.select(tweets['created_at']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.filter(tweets['followers_count'] > 500000) \\\n",
    "    .select(tweets['screen_name'], tweets['followers_count']) \\\n",
    "    .orderBy(tweets['followers_count'], ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can write the same thing somewhat more compactly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.filter('followers_count > 500000') \\\n",
    "    .select('screen_name', 'followers_count') \\\n",
    "    .orderBy('followers_count', ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results are exactly the same!  How you write your code is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SQL with DataFrames\n",
    "\n",
    "All you need to do to get going with SQL is to register a table from your data frame, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"SELECT COUNT(*) FROM tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"SELECT COUNT(*) FROM tweets\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"\"\"\n",
    "    SELECT followers_count \n",
    "    FROM tweets\n",
    "    ORDER BY followers_count DESC\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"\"\"\n",
    "    SELECT screen_name\n",
    "    FROM tweets\n",
    "    WHERE followers_count > 5000000\n",
    "    ORDER BY screen_name\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other data types:  JSON\n",
    "\n",
    "We can load in non-CSV data as well, such as JSON.  Here is a set of tweet data in JSON format, the original source.  It's much less likely to have wrangling issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/week-9/mlb-world-series/9670f3399f774789b7c3e18975d25611_001.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv 9670f3399f774789b7c3e18975d25611_001.json mlb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l mlb.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 mlb.json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON data is pretty common these days, and Python makes it easy to work with.  Here's what it looks like from Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -1 mlb.json > mlb1.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "mlb = json.load(open(\"mlb1.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb['user']['screen_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb['user']['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(mlb, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's a tour of one tweet.  Let's look at a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = sqlc.read.json(\"mlb.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = mlb.sample(False, 0.1, 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is **hierarchy** in JSON structures like tweets.  We can use `.` to address this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.orderBy(\"user.followers_count\", ascending=False).select('user.name').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample = mlb.sample(False, 0.01, 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample.orderBy(\"user.followers_count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sample.rdd.flatMap(lambda r: r['text'].split(' ')) \\\n",
    "    .map(lambda t: (t, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rdd.flatMap(lambda r: r['text'].split(' ')) \\\n",
    "    .map(lambda t: (t, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(10, key=lambda pair: -pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, we can do things like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.createOrReplaceTempView(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"SELECT * FROM sample\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlc.sql(\"\"\"\n",
    "    SELECT user.screen_name, user.followers_count AS fc\n",
    "    FROM sample\n",
    "    ORDER BY fc DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib example\n",
    "\n",
    "### Fetch and prepare data - Iris classification\n",
    "\n",
    "Let's use a classic dataset, [Fisher's Iris data](https://en.wikipedia.org/wiki/Iris_flower_data_set).  The best source for this is at its [UCI repository page](https://archive.ics.uci.edu/ml/datasets/iris), where they have a useful [descriptive page](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names).  Every ML library has a tutorial that uses this dataset.\n",
    "\n",
    "The purpose of reviewing this dataset isn't to demonstrate Spark's computing power; we've already seen that.  The goal here is rather to show you how to produce a machine learning model using Spark.  You can compare the process to what you can find with environments like R, scikit-learn, and others.  The process compares pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/2018-dmfa/week-13/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head iris.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!csvstat iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = sqlc.read.csv('iris.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "If you haven't encountered this dataset before, get used to it - you're sure to see it again.\n",
    "\n",
    "Let's have a quick look before we dig in to get a visual sense of the data and the spreads of its independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib's [style options](https://matplotlib.org/devdocs/gallery/style_sheets/style_sheets_reference.html) are fun to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_iris = iris.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_iris.hist(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter_matrix(pd_iris, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepping features\n",
    "\n",
    "It is common to go through stages of feature engineering that involve transformation, scaling, indexing, and similar steps.  MLlib provides a `Pipeline` to assemble these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we pull our features into a feature vector using a `VectorAssembler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\",\n",
    "                                       \"petal_length\", \"petal_width\"],\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to have our various target labels in `class` indexed to numeric category values.  The MLlib `StringIndexer` can turn these string values into a numeric representation of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"labelIndex\").fit(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prep our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"labelIndex\", featuresCol=\"features\", numTrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're going to need to be able to get those class labels back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeler = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                        labels=indexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assemble all of these steps into a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler, indexer, rf, labeler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "We always want to hold out some data from training so we can get an honest assessment on how our model will perform with unseen data.  Fortunately there's an easy API call for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(iris.randomSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = iris.randomSplit([0.6, 0.4], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good.  We'll use `train` to build our model, and leave `test` alone for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"predictedLabel\", \"class\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['accuracy', 'f1']:\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\",\n",
    "                                                  predictionCol=\"prediction\",\n",
    "                                                  metricName=metric)\n",
    "    print(\"{}: {}\".format(metric, evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = model.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in s2.trees:\n",
    "    print(t.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test stuff for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_sample = sqlc.read.json(\"*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_sample.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeppelin code for EMR cluster\n",
    "\n",
    "Let's see what Spark can do on a real cluster.  We'll run some very similar queries on an AWS EMR cluster after break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "from pyspark import SQLContext\n",
    "sqlc = sqlc = SQLContext(spark)\n",
    "mlb = sqlc.read.json(\"s3://2018-dmfa/week-9/2018-world-series/*.json\")\n",
    "mlb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pyspark\n",
    "mlb.createOrReplaceTempView(\"mlb\")\n",
    "sqlc.sql(\"\"\"\n",
    "    SELECT user.screen_name AS name, user.followers_count AS fc\n",
    "    FROM mlb\n",
    "    WHERE user.followers_count > 5000000\n",
    "    GROUP BY user.screen_name, user.folowers_count\n",
    "    ORDER BY user.followers_count DESC\n",
    "\"\"\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
